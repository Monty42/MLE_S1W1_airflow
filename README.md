# Вебинар "Создание Airflow DAG разными способами"

## Курс "Инженер машинного обучения", Яндекс Практикум

>### Описание проекта

Подготовка данных для последуещего применения в моделях машинного обучения.

1. Рассмотрим EDA для данных Всемирного банка (World Bank)
2. Реализуем ETL-пайплайна этих данных
3. Создадим первый DAG для получившегося ETL-пайплайна
4. Попрактикуемся
5. Создадим альтернативную реализацию DAG

>### Как работать с проектом?

>#### 1. Скачивание и установка зависимостей

1. Чтобы редактировать репозиторий, необходимо сделать его копию. Это делается по кнопке fork.

2. Клонируем репозиторий на своё локальное устройство (но лучше все-таки на виртуальное устройство):

~~~
git clone <адрес вашей копии репозитория>
~~~

3. Создаём новую ветку:

~~~
git checkout -b <имя новой ветки>
~~~

4. Установка нужных библиотек и окружения:

~~~
python -m venv <имя папки для хранения окружения>
source <имя папки для хранения окружения>/bin/activate
python -m pip install --upgrade pip
pip install -r requirements.txt
~~~

5. Создайте `.env` файл на основе файла `.env_template`. Вы должны были получить доступ к базам данных во время выполнения первых модулей курса.

6. Готово! Можно проверить работоспособность окружения.

#### 2. Запустим каждый этап по порядку

> ##### 2.1 Запуск ETL-пайплайна

Небольшой анализ данных и выводы по работе с ними вы можете посмотреть в файле `EDA.ipynb`.

ETL-пайплайн для работы с данными находиться в файле `wb_data_etl.py`.
Для запуска пайплайна зайдите в ноутбук `test.ipynb` -- вы можете использовать его для проверки заполнения ваших баз данных.

Для установки ядра Jupyter, запустите команду:

~~~
jupyter notebook
~~~

После этого выберите ядро для данного ноутбука или перейдите по ссылке в терминале.

Запустите ячейки ноутбука.

Когда вы выполните первую ячейку, в директории проекта появится файл `py_log.log`.
После выполнения второй ячейки данный файл заполниться логами. Убедитесь, что статус выполнения в последней строке `OK`

После этого вы можете проверить свою целевую базу данных с помощью библиотеки `SQLAlchemy`. Примеры использования можно посмотреть в файле `wb_data_etl.py`

> ##### 2.2 Запуск Airflow

Скачайте официальный образ сервисов Airflow

~~~
curl -LfO https://airflow.apache.org/docs/apache-airflow/2.7.3/docker-compose.yaml
~~~

В скачанном файле `docker-compose.yaml` раскоментируйте 54 строку и закоментируйте 53 строку как показано ниже:

~~~
53.    # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}
54.    build: .
~~~

Если у вас не инициализированы папки для работы Airflow (внутри скачанного репозитория есть все папки кроме `./logs`), выполните следующую команду в терминале:

~~~
mkdir -p ./dags ./logs ./plugins ./config
~~~

Добавьте информацию о вашем `AIRFLOW_UID` в файл `.env`

~~~
echo -e "\nAIRFLOW_UID=$(id -u)" >> .env
~~~

Следующая команда создаст учетную запись с логином и паролем Airflow для веб-интерфйса

~~~
docker compose up airflow-init
~~~

Отчистим возможный кэш, который создался на прошлом шаге

~~~
docker compose down --volumes --remove-orphans
~~~

Начнем сборку образа с всеми параметрами

~~~
docker compose up --build
~~~

- **Логин**: airflow
- **Пароль**: airflow

> ##### 2.3 Подключение БД

Вам необходимо подключить 3 ДБ:

1. `source_db` -- общая база данных студентов
2. `destination_db` -- персональная БД, в которые вы можете выгружать данные
3. `population_db` -- локальная SQLite ДБ, которую мы с помощью Dockerfile положили в папку `./tmp`

Как подключить первые 2 БД показана внутри 1 спринта, поэтому мы расскажем только о том как подключить последнюю:
![image](/for_readme/population_data.png)

Заполните поля как показано на рисунке. Сохраните изменения

После добавления у вас в меню Connections будет подобная картина:
![image](/for_readme/All_hooks.png)

> ##### 2.4 Запустим TaskFlow DAG

На общей странице DAGs вы можете увидить подгрузившиеся даги:

![image](/for_readme/DAGs.png)

Выберите даг с названием `prepare_wb_data`.
![image](/for_readme/TaskFlow.png)

Вы попадете на страницу, где можно выполнить этот даг. После выполнения вы увидите что-то подобное:

![image](/for_readme/TaskFlow_result.png)

Код для этого DAG лежит внутри папки `dags`, с названием `wb_data.py`.

Проверьте Логи и выходную таблицу. Есть ли там данные и похожи ли они на то, что мы уже проделали в ETL.

> ##### 2.5 Запустим альтернативный DAG

На общей странице DAGs вы можете увидить подгрузившиеся даги:

![image](/for_readme/DAGs.png)

Выберите даг с названием `prepare_wb_data_alt`.
![image](/for_readme/WithFlow.png)

Вы попадете на страницу, где можно выполнить этот даг. После выполнения вы увидите что-то подобное:

![image](/for_readme/WithFlow_result.png)

Код для этого DAG лежит внутри папки `dags`, с названием `alt_wb_data.py`.

Проверьте Логи и выходную таблицу. Есть ли там данные и похожи ли они на то, что мы уже проделали в ETL и предыдущем DAG.

#### 3. Задание для самостоятельного выполнения

> ##### 3.1 На занятии (Время выполнения +- 20 минут)

Реализауем DAG до конца на основе построенного ранее ETL-пайплайна.

1. Запустите airflow по инструкции описанной выше

2. На основе ETL-пайплайна реализуйте таски преобразования (`transform`) и выгрузки данных (`load`)

3. Запустите папйлан через интерфейс Airflow. Не забудьте про логирование, это поможет вам быстрее обнаружить ошибку в случае ее возникновения. Файл test.ipynb может помочь вам оценить результаты выполнения пайплайна.

4. Измените параметры запуска таким образом, чтобы весь пайплайн мог завершиться с ошибкой не более 3 раз (не забудьте про периоды перезапуска)

5. Сделайте скриншот выполнения DAG в Airflow и скриншот выходной таблицы. Добавьте скриншоты в корень проекта (или создайте кастомную папочку).

6. Сделайте pull request в master и скиньте на него ссылку в чат.

> ##### 3.2 В качестве дополнительной практики

В ходе самостоятельной работы вам необходимо модифицировать имеющийся DAG:

1. Необходимо добавить в выходную таблицу данные изменения численности и доступность электроэнергии для населения как дополнительные фичи:
    - `s1w1_electricity_access_percent` - таблица "доступность электроэнергии для населения" в базе `source`
    - `s1w1_rural_population_percent` - таблица "изменения численности" в базе `source`

2. Преобразуйте данные стоимости проекта `totalamt` в числовой формат данных.

3. Измените схему финальной таблицы в соответствии с добавленными полями

4. Измените параметры запуска таким образом, чтобы весь пайплайн мог завершиться с ошибкой не более 3 раз (не забудьте про периоды перезапуска)

5. Сделайте скриншот выполнения DAG в Airflow и скриншот выходной таблицы. Добавьте скриншоты в корень проекта (или создайте кастомную папочку).

6. Сделайте pull request в master и скиньте на него ссылку в чат.

> ##### 3.3 В качестве дополнительного задания

1. Попробуйте реализовать `TelegramOperator` для оповещений.
2. Поменяйте базовые параметры DAG. Попробуйте добавить параметры `retries`, `retries_delay` или `retry_exponential_backoff`.
3. Попробуйте изменить последовательность выполнения операций через разные

#### 4. Шпаргалка для работы с гитом

1. Добавляем изменения в индекс:

~~~
git add .
~~~

2. Создаём коммит:

~~~
git commit -m "Описание того, что было изменено"
~~~

3. Отправляем изменения в удалённый репозиторий:

~~~
git push
~~~

Если вы локально создали новую ветку, эта команда не сработает, но git сам подскажет, как её модернизировать, чтобы добавить ветку. Можно просто скопировать :)

#### 5. Архитектура проекта

==============================

    ├── README.md 
    │         
    ├── data               <- Папка c локальными данными
    │
    ├── notebooks          <- Ноутбук с разведочным анализом данных
    │
    ├── configs            <- Конфиги для работы с Airflow
    │
    ├── logs               <- Логи выполнения задач Airflow
    │
    ├── requirements.txt   <- Файл с зависимостями
    │
    ├── dags               <- Здесь будут лежать ваши DAG
    │
    ├── plugins   <- Код веб-приложения для онлайн-использования модели
    │   │
    │   └── steps 
    │       ├── __init__.py    <- Необходимо для восприятия директории как модуля Python  
    │       └── wb_data.py     <- Необходимые шаги, которые будут использоваться в реализации альтернативного DAG
    │
    ├── wb_data_etl.py     <- Код запуска ETL-пайплайна
    │
    ├── .env_template      <- Файл с параметрами вашего окружения. Необходимо создать файл `.env` или переименовать этот файл и добавить в него свои параметры.
    │
    └── Dockerfile         <- Код запуска контейнера с Airflow             

-----------------

#### 6. В случае ошибок

Так как ошибки неизбежны, предлагаю выполнять следующие действия:

1. Погуглите, скорее всего кто-то уже пытался это решить.

2. Если не понимаете, что нагуглили, или не нагуглили вовсе, предлагаю 2 доступных варианта:

    - создать тему с обсуждением на GitHub и приложить свою ошибку, мы постараемся вам помочь.
    - Создайте PullRequest в `main` и внутри описания изменений напишите свою ошибку, тогда мы сможем помочь оперативнее

> ## Мы будем рады, если данный код поможет вам попрактиковать свои навыки в работе с AirFlow и запуском сторонних репозиториев 🥸
